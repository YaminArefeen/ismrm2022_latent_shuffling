{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import time \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_model = 'tse'     #signal model to experiment on \n",
    "\n",
    "model_type     = 2\n",
    "latent_nums    = [1,2,3,4]\n",
    "learning_rates = [1e-4,1e-4,1e-4,1e-4]\n",
    "all_epochs     = [200000,200000,200000,200000]\n",
    "\n",
    "test_skip_idx = 10   #grab every 10th entry of dictionary for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the baseline dictionary\n",
    "dictionary = sio.loadmat('data/dictionary_%s.mat' % signal_model)['dictionary']\n",
    "t2_range   = sio.loadmat('data/dict_params_%s.mat' % signal_model)['dict_params'][0][0][0]\n",
    "    \n",
    "t2_range_training = np.expand_dims(np.delete(t2_range,slice(None,None,test_skip_idx)),axis=0)\n",
    "t2_range_testing  = t2_range[:,::test_skip_idx]\n",
    "\n",
    "dictionary_training = np.delete(dictionary,slice(None,None,test_skip_idx),axis=1)\n",
    "dictionary_testing  = dictionary[:,::test_skip_idx]\n",
    "    \n",
    "[T,Ntraining] = dictionary_training.shape\n",
    "[_,Ntesting]  = dictionary_testing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining auto-encoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self,input_dimension,hidden_dimension):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_fc_1  = nn.Linear(input_dimension,input_dimension//2)\n",
    "        self.input_fc_2  = nn.Linear(input_dimension//2,hidden_dimension)\n",
    "        self.output_fc_1 = nn.Linear(hidden_dimension,input_dimension//2)\n",
    "        self.output_fc_2 = nn.Linear(input_dimension//2,input_dimension)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = F.leaky_relu(self.input_fc_1(x))\n",
    "        out = F.leaky_relu(self.input_fc_2(out))\n",
    "        out = F.leaky_relu(self.output_fc_1(out))\n",
    "        out = self.output_fc_2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#re-shaping dictionary, casting it to right type, and sending it to GPU\n",
    "dictionary_for_training = torch.tensor(np.transpose(dictionary_training,(1,0)),dtype = torch.float).to(device)\n",
    "\n",
    "models = []\n",
    "\n",
    "for latent_num,epochs,learning_rate in zip(latent_nums,all_epochs,learning_rates):\n",
    "    starting_time = time.time()\n",
    "    \n",
    "    print('Latent Variables: %d || epochs: %d || learning rate: %.6f' % (latent_num,epochs,learning_rate))\n",
    "    model = autoencoder(T,latent_num)\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(dictionary_for_training)\n",
    "        loss   = criterion(output,dictionary_for_training)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss = loss.item()\n",
    "        \n",
    "        if(epoch % 1000 == 0):\n",
    "            print('  current loss: %.12f' % running_loss)\n",
    "    \n",
    "    print('  final loss: %.12f' % running_loss)\n",
    "    ending_time = time.time()\n",
    "    print('  elapsed time: %.2f min' % ((ending_time - starting_time)/60))\n",
    "\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute training and testing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = lambda truth, comp: np.sqrt(np.sum((comp - truth)**2,keepdims=False,axis=0)) / np.sqrt(np.sum(truth**2,keepdims=False,axis=0))\n",
    "\n",
    "[u,s,v] = np.linalg.svd(dictionary,full_matrices=False)\n",
    "\n",
    "all_basis = []\n",
    "\n",
    "linear_errors_training   = []\n",
    "proposed_errors_training = []\n",
    "\n",
    "linear_errors_testing   = []\n",
    "proposed_errors_testing = []\n",
    "\n",
    "dictionary_linear_training   = []\n",
    "dictionary_proposed_training = []\n",
    "\n",
    "dictionary_linear_testing   = []\n",
    "dictionary_proposed_testing = []\n",
    "\n",
    "for latent_num, model in zip(latent_nums,models):\n",
    "    #determine basis\n",
    "    basis = u[:,0:latent_num]\n",
    "    \n",
    "    #estimate dictionaries with svd and autoencoder\n",
    "    dictionary_estimate_linear_training   = basis @ np.conj(basis.T) @ dictionary_training\n",
    "    dictionary_estimate_proposed_training = model(dictionary_for_training).cpu().detach().numpy().transpose((1,0))\n",
    "    \n",
    "    dictionary_estimate_linear_testing    = basis @ np.conj(basis.T) @ dictionary_testing\n",
    "    dictionary_for_testing                = \\\n",
    "        torch.tensor(np.transpose(dictionary_testing,(1,0)),dtype = torch.float).to(device)\n",
    "    dictionary_estimate_proposed_testing = \\\n",
    "        model(dictionary_for_testing).cpu().detach().numpy().transpose((1,0))\n",
    "\n",
    "    error_linear_training   = rmse(dictionary_training,dictionary_estimate_linear_training)\n",
    "    error_proposed_training = rmse(dictionary_training,dictionary_estimate_proposed_training)\n",
    "    \n",
    "    error_linear_testing    = rmse(dictionary_testing,dictionary_estimate_linear_testing)\n",
    "    error_proposed_testing  = rmse(dictionary_testing,dictionary_estimate_proposed_testing)\n",
    "    \n",
    "    linear_errors_training.append(error_linear_training)\n",
    "    proposed_errors_training.append(error_proposed_training)\n",
    "    \n",
    "    linear_errors_testing.append(error_linear_testing)\n",
    "    proposed_errors_testing.append(error_proposed_testing)\n",
    "    \n",
    "    dictionary_linear_training.append(dictionary_estimate_linear_training)\n",
    "    dictionary_proposed_training.append(dictionary_estimate_proposed_training)\n",
    "    \n",
    "    dictionary_linear_testing.append(dictionary_estimate_linear_testing)\n",
    "    dictionary_proposed_testing.append(dictionary_estimate_proposed_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot entry by entry training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for latent_num,error_linear,error_proposed in zip(latent_nums,linear_errors_training,proposed_errors_training):\n",
    "    plt.title('Latent Variables: %d' % latent_num,fontsize=24)\n",
    "    plt.plot(error_proposed)\n",
    "    plt.plot(error_linear)\n",
    "    plt.legend(['proposed','linear'],prop={'size':20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot entry by entry testing error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lw = 3\n",
    "for latent_num,error_linear,error_proposed in zip(latent_nums,linear_errors_testing,proposed_errors_testing):\n",
    "    print('average percent errors:')\n",
    "    print('  proposed: %.2f' % (np.mean(error_proposed)*100))\n",
    "    print('  linear:   %.2f' % (np.mean(error_linear)*100))\\\n",
    "    \n",
    "    plt.title('Latent Variables: %d' % latent_num,fontsize=24)\n",
    "    plt.plot(error_proposed,linewidth = lw)\n",
    "    plt.plot(error_linear,  linewidth = lw)\n",
    "    plt.legend(['proposed','linear'],prop={'size':20})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
